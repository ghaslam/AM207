<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                    <meta name="description" content="">
                        <meta name="author" content="">
                            <link rel="shortcut icon" href="img/favicon.ico">
                                
                                <title>AM 207 Final Project</title>
                                
                                <!-- Bootstrap core CSS -->
                                <link href="//netdna.bootstrapcdn.com/bootswatch/3.1.1/yeti/bootstrap.min.css" rel="stylesheet">
                                    
                                    <!-- Custom styles for this template -->
                                    <link href="http://getbootstrap.com/examples/jumbotron-narrow/jumbotron-narrow.css" rel="stylesheet">
                                        
                                        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
                                        <!--[if lt IE 9]>
                                         <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
                                         <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
                                         <![endif]-->
                                        
                                        <script type="text/x-mathjax-config">
                                            MathJax.Hub.Config({
                                                               tex2jax: {inlineMath: [['$','$']]}
                                                               });
                                            </script>
                                        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
                                            </script>
                                        </head>
    
    <body>
        
        <div class="container">
            <div class="header">
                <ul class="nav nav-pills pull-right">
                    <li class="active"><a href="index.html">Home</a></li>
                    <li><a href="simulation.html">PREDICTION</a></li>
                    <li><a href="optimization.html">BEST ROUTE</a></li>
                    <li><a href="https://github.com/ghaslam/AM207/" target="_blank">
                        <img src="img/GitHub-Mark-32px.png" height="18" style="vertical-align:text-top;"/> Code</a></li>
                </ul>
                <h3 class="text-muted">AM 207 Final Project</h3>
            </div>
            
            <div class="container">
                <h2>Predicting water pump failure in Tanzania and optimising maintenance routes</h2>
                <h3 class="text-muted">Predicting functional status of water pumps using Bayesian Methods</h3>
            </div>
            
            <div class="row marketing">
                <div class="col-md-12">
                    
                    <h4>Model Classification</h4>
                    
                    <p>Bayesian methods are only compatible with numerical data and do not work directly with classification problems, so we need to convert the categorical data and classes into numerical data and labels.</p>
                    
                    <p>The assumption that the 3 labels lie on a linear spectrum is not necessarily a safe assumption, and the limits are set rather arbitrarily. A more natural and common method in machine learning is to build a classifier that decides between 2 classes. In this case, we can either build 3 one-versus-one classifiers or 3 one-versus-rest classifiers and use majority vote (or probabilities in the case of a tie) to make the final classification.</p>
                    
                    <h4>Categorical data</h4>
                    
                    <p> Many of the data features are categorical data that do not have any numerical interpretation, so we cannot convert them the same way that we converted our label. For example, the "installer" feature has labels such as "UNICEF," "Roman," and "Artisan." To deal with these, we use one-of-k representation in which we add a new feature column for every unique value. For example, we would add the columns "installer=UNICEF," "installer=Roman," and "installer=Artisan." The "installer=UNICEF" would be 1 if that data's "installer" was "UNICEF," and 0 otherwise. So if there are $k$ unique values for a particular categorical feature, then for each data point, we add on a vector of length $k$ which has a single 1 and $k-1$ 0s.</p>
                    
                    <h4>The model</h4>
                    
                    <p>We model $y_i$ using a logistic function that constrains the range to $0 < y_i < 1$, and the parameter for the logistic function is controlled by the features and weights, which are also parameters that we need to calculate. Our model comes from the assumption that there are certain factors impacting decay rate, and so the functionality is dependent on this decay rate. We then select features from the data which we have a prior belief to have a strong influence on functionality, choose priors on their distributions and construct a Metropolis-Hastings Sampler to select the optimum value for each parameter. New predictions of functionality can then be made from this model. We also add on a term for noise, $\epsilon_i$, which we assume is Gaussian noise controlled by the standard deviation $\sigma$. </p>
                    
                    $$\begin{aligned}
                    y_i &= \frac{1}{1+e^{\alpha_i}}+\sigma\epsilon_i \\
                    \alpha_i &= \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + ... + \beta_n x_{i,n}\end{aligned}$$
                    
                    <p>There are $n$ features and $n+2$ parameters ($n+1$ $\beta$s and $\sigma$). The parameters are found by sampling from the posterior: </p>
                    $$\begin{aligned}
                    p(Y,\Theta)=p(Y|\Theta)p(\Theta)\end{aligned}$$
                    
                    <h4>Prediction results</h4>

                    <p>Figures <a href="#2">2</a> and <a href="#3">3</a> shows shows the trace plot of the model parameters with blockwise updating and componentwise updating. These are after the burn-in period. Blockwise updating has an acceptance rate of nearly 0 because we are updating 8 parameters at a time. Componentwise updating has a much higher acceptance rate, although at a huge time cost.</p>
                    
                    <div class="panel panel-info">
                    <a name="2"></a>
                    <div class="panel-heading">Figure 2</div>
                    <div class="panel-body">
                    <img src="img/trace_block.png" width="100%" max-width="600" />
                    </div>
                    <div class="panel-footer">Blockwise updating.</div>
                    </div>
                    
                    <div class="panel panel-info">
                        <a name="3"></a>
                        <div class="panel-heading">Figure 3</div>
                        <div class="panel-body">
                            <img src="img/trace_component.png" width="100%" max-width="600" />
                        </div>
                        <div class="panel-footer">Componentwise updating.</div>
                    </div>
                    


                    <p>The parameters chosen for the model are "longitude," "latitude," "age" (2015-"construction\_year"), "gps\_height," "quantity=dry," and "population." We see that $\beta_5$ has significant predictive power since this is the farthest from 0. This corresponds to the "quantity=dry" feature. $\beta_0$ is the constant offset, which we would expect to be non-zero. By fine-tuning the step parameters and carefully choosing features, we reach a cross-validation prediction accuracy of 60.6%.</p>
                    
                    <h4>Comparison to Machine Learning Approaches</h4>
                    
                    <p>Here we compare our methods with other common machine learning methods. The k-Nearest Neighbors algorithm (also known as the k-NN algorithm) is a machine learning algorithm that can be applied to classification problems. There is no training aspect to the algorithm. Predictions are made by taking the $k$ nearest data points to the desired data point in feature space, and taking the majority of their labels. The labels can also be weighted by $1/d$, where $d$ is the distance from the desired data point to the labeled data point. We apply the k-NN algorithm using only the longitude and latitude data, so we are picking the geographically nearest neighbors. Accuracy in cross-validation is shown in the Figure <a href="#4">4</a> as a function of $k$. We see that it reaches 69% prediction accuracy.</p>
                    
                    <div class="panel panel-info">
                    <a name="4"></a>
                    <div class="panel-heading">Figure 4</div>
                    <div class="panel-body">
                    <img src="img/kNN_accuracy_vs_K.png" width="100%" max-width="600" />
                    </div>
                    <div class="panel-footer">Effect of increasing number of nearest neighbors on accuracy.</div>
                    </div>
                    
                    <h4>Conclusions </h4>
                    
                    <p>Decision trees are a machine learning method in which a tree of rules is built. One traverses down the tree, and each node represents a rule based on the data's features. The leaves are the final decision. The nodes are built based on which features have the most predictive power for classification. Random forests are an extension of decision trees in which many trees are trained, and stochasticity is introduced to make the trees different, which decreases bias. We show in randomforest\_explore.ipynb that random forests on a small subset of features can reach an accuracy of 79\%. Further experimentation and more sophisticated feature extraction can reach prediction accuracies of 83% on this data. </p>
                    
                    <p> Bayesian modeling is probably not the best method to use for this type of problem. It assumes that the classification can be described by some sort of model, and we would have to guess this model ahead of time. Decision trees and random forests do not assume any sort of relation between the variable features and the labels, and merely seek out the variable features with the highest predictive power. Second, there are a lot of parameters in the MH sampler that need to be tuned, including initial guesses for the parameters and step sizes. Additionally, Bayesian modeling takes extremely long to train, on the order of hours on a laptop, assuming that the initial guesses for the parameters and step sizes are set correctly. Training k-NN and random forests take on the order of minutes. </p>
                    
                    </div>
                    </div>
                    
                    <div class="footer">
                    <p>S. Kim, G. Haslam, 2015</p>
                    </div>
                    
                    </div> <!-- /container -->
                    
                    
                    <!-- Bootstrap core JavaScript ================================================== -->
                    <!-- Placed at the end of the document so the pages load faster -->
                    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
                    <script src="https://code.jquery.com/jquery.js"></script>
                    <!-- Include all compiled plugins (below), or include individual files as needed -->
                    <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.2/js/bootstrap.min.js"></script>
                    </body>
                    </html>