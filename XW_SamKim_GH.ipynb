{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##AM207 Project Proposal Predicting Water Well Failure\n",
    "###Team Members: Sam Kim, Harvard College '15, and Gareth Haslam, Ext. School\n",
    "###Date: 10th April 2015\n",
    "\n",
    "Predicting failure of water wells is an important issues for millions of people in developing countries who rely on these wells for their clean drinking water. Using the dataset available as part of the DrivenData.org challenge we aim to use Bayesian Methods to investigate how to better predict failure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####Introduction\n",
    "\n",
    "We have decided to tackle the challenge presented by DrivenData at http://www.drivendata.org/competitions/7/page/23/. The data is given by Taarifa, which is an open source platform for reporting infrastructure issues, and the Tanzanian Ministry of Water. The goal is to \"predict the operating condition of a waterpoint for each record in the dataset.\" The possible classifications are \"functional,\"  \"non functional,\" and \"functional needs repair.\" Being able to predict which waterpoints will fail will improve maintenance operations and infrastructure upkeep to ensure that communities across Tanzania have access to clean water without spending resources constantly monitoring each waterpoint.\n",
    "\n",
    "Because this is presented as a challenge by DrivenData, there have not been many previous work on this problem. In addition, the open source data platform, Taarifa, only entered Tanzania relatively recently, so there is still a low of work to do. Bayesian methods in general have been applied successfully to other classification problems, such as Box Office Prediction (http://cs229.stanford.edu/proj2013/cocuzzowu-hitorflop.pdf), even better than machine learning techniques such as SVMs. We hope to use similar techniques to this problem.\n",
    "\n",
    "In addition to simply predicting the functionality of the water wells, we hope to also provide insight into how the various attributes relate to the functionality of the water well. Bayesian techniques allows us to build models, learn the parameters for the models, and then find probability distributions for the dependent variable. This is one of the advantages of Bayesian techniques compared to other machine learning techniques, which are often black box models that do not provide intuitive insight into how the parameters relate to the dependent variable, and simply give an answer rather than a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Data\n",
    "\n",
    "The data has been provided by the challenge, and includes 39 different variables on funding of the well, what kind of pump is operating, its location, when it was installed, population data around the well, qauntity of water, cost of water, and how it is managed. There are 59400 data points available for training, which is more than enough for meaningful cross-validation tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Methodology\n",
    "\n",
    "We plan on using Bayesian modeling to predict the functional status of each waterpoint as a function of all the variables.\n",
    "\n",
    "Naive Bayes gives us:\n",
    "\n",
    "$$P(status|\\Theta)\\propto P(\\Theta|status)P(status)$$\n",
    "\n",
    "where $\\Theta$ describes the attributes from our data. In the simplest case, it can simply be a product of the conditional probability for each attribute, $P(\\theta_i|status)$. If we want to build a more complex model, we would include parameters $\\alpha_i$ to relate the attributes to each other and to the status, such as $P(broken|age)=\\alpha_1+\\alpha_2\\cdot age$. Because we have very little experience in this field and know little about the model, the parameters $\\alpha_i$ would also have distributions with hyperparameters, $\\beta_i$. This chain can extend as far as we want. \n",
    "\n",
    "In the end, we are sampling from the joint posterior distribution for all the parameters, $\\theta_i, \\alpha_i, \\beta_i, ...$, which is $P(\\Theta, \\alpha, \\beta,... | status)$ to build the model, and then using this distribution to predict the status for unknown data.\n",
    "\n",
    "Sampling the joint posterior distribution can be done through any of the methods taught in class, including Metropolis-Hastings and its numberous variants, Gibbs, slice sampling, and so on.\n",
    "\n",
    "Part of the challenge becomes building meaningful models. Machine learning techniques in dimension reduction such as PCA can be used to identify the most meaningful attributes and how they relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
